# Mini-ANNs Documentation

Welcome to the Mini-ANNs project documentation! This comprehensive guide will help you understand, set up, and use the various neural network experiments in this project.

## ğŸ“š Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Experiments Guide](#experiments-guide)
- [API Documentation](#api-documentation)
- [Dashboard Usage](#dashboard-usage)
- [Contributing](#contributing)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

Mini-ANNs is a comprehensive collection of minimal artificial neural network experiments designed for educational purposes. Each experiment demonstrates specific concepts in deep learning using PyTorch, with working code that runs without modification.

### Key Features

- **15 Complete Experiments**: From basic classification to advanced GANs
- **Interactive Dashboard**: Streamlit-based web interface
- **RESTful API**: Flask-based API for model inference
- **Comprehensive Documentation**: Detailed explanations and tutorials
- **Ready-to-Run Code**: All experiments work out of the box
- **Educational Focus**: Clear explanations and learning objectives

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- Jupyter Notebook
- Git

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd mini-anns
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv mini-anns-env
   source mini-anns-env/bin/activate  # On Windows: mini-anns-env\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run your first experiment:**
   ```bash
   jupyter notebook notebooks/01_tiny_image_classifier.ipynb
   ```

### Quick Test

```python
# Test installation
import torch
import torchvision
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

## ğŸ“ Project Structure

```
mini-anns/
â”œâ”€â”€ README.md                 # Main project documentation
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ data/                    # Dataset storage
â”‚   â”œâ”€â”€ mnist/              # MNIST data
â”‚   â”œâ”€â”€ fashion-mnist/      # Fashion-MNIST data
â”‚   â””â”€â”€ cifar10/            # CIFAR-10 data
â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_tiny_image_classifier.ipynb
â”‚   â”œâ”€â”€ 02_mini_autoencoder.ipynb
â”‚   â”œâ”€â”€ 03_micro_lstm.ipynb
â”‚   â”œâ”€â”€ 04_mini_time_series.ipynb
â”‚   â”œâ”€â”€ 05_anomaly_detection.ipynb
â”‚   â”œâ”€â”€ 06_mini_cnn.ipynb
â”‚   â”œâ”€â”€ 07_pruning_study.ipynb
â”‚   â”œâ”€â”€ 08_toy_problems.ipynb
â”‚   â”œâ”€â”€ 09_mini_gan.ipynb
â”‚   â”œâ”€â”€ 10_energy_efficient_ann.ipynb
â”‚   â”œâ”€â”€ 11_activation_comparison.ipynb
â”‚   â”œâ”€â”€ 12_regularization_practice.ipynb
â”‚   â”œâ”€â”€ 13_learning_rate_experiments.ipynb
â”‚   â”œâ”€â”€ 14_data_size_vs_accuracy.ipynb
â”‚   â””â”€â”€ 15_transfer_learning_mini.ipynb
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ utils.py           # Helper functions
â”‚   â”œâ”€â”€ train.py           # Training utilities
â”‚   â””â”€â”€ evaluate.py        # Evaluation utilities
â”œâ”€â”€ app/                   # Web applications
â”‚   â”œâ”€â”€ streamlit_app.py   # Streamlit dashboard
â”‚   â””â”€â”€ gradio_app.py      # Gradio interface
â”œâ”€â”€ api/                   # API server
â”‚   â”œâ”€â”€ app.py            # Flask API
â”‚   â””â”€â”€ model.pth         # Example saved model
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ index.md          # This file
â”‚   â””â”€â”€ experiments.md    # Detailed experiment guide
â””â”€â”€ results/              # Output directory
    â”œâ”€â”€ plots/            # Generated plots
    â””â”€â”€ logs/             # Training logs
```

## ğŸ§ª Experiments Guide

### Core Experiments (1-10)

1. **Tiny Image Classifier** - Basic MNIST classification
2. **Mini Autoencoder** - Image compression and reconstruction
3. **Micro LSTM** - Character-level text generation
4. **Mini Time Series** - LSTM-based forecasting
5. **Anomaly Detection** - VAE for outlier detection
6. **Mini CNN** - Convolutional neural networks
7. **Pruning Study** - Network compression techniques
8. **Toy Problems** - Classic ML problems (XOR, spirals)
9. **Mini GAN** - Generative adversarial networks
10. **Energy Efficient ANN** - Model optimization

### Enhancement Experiments (11-15)

11. **Activation Comparison** - ReLU vs Sigmoid vs Tanh
12. **Regularization Practice** - Dropout, L2, early stopping
13. **Learning Rate Experiments** - LR scheduling and optimization
14. **Data Size vs Accuracy** - Dataset size impact analysis
15. **Transfer Learning Mini** - Fine-tuning pre-trained models

### Running Experiments

Each notebook is self-contained and can be run independently:

```bash
# Run specific notebook
jupyter notebook notebooks/01_tiny_image_classifier.ipynb

# Run all notebooks (in sequence)
for notebook in notebooks/*.ipynb; do
    jupyter nbconvert --execute --to notebook --inplace "$notebook"
done
```

## ğŸŒ API Documentation

### Starting the API Server

```bash
cd api
python app.py
```

The API will be available at `http://localhost:5000`

### Available Endpoints

#### General Endpoints
- `GET /` - API information
- `GET /health` - Health check
- `GET /models` - List available models

#### Prediction Endpoints
- `POST /predict/classifier` - Image classification
- `POST /predict/autoencoder` - Image reconstruction
- `POST /predict/timeseries` - Time series forecasting
- `POST /predict/cifar10` - CIFAR-10 classification

#### Model Management
- `GET /models/<name>/info` - Model details
- `POST /models/<name>/save` - Save model
- `POST /models/<name>/load` - Load model

### Example API Usage

```python
import requests
import base64
from PIL import Image
import io

# Prepare image
image = Image.open('sample.png').convert('L').resize((28, 28))
buffer = io.BytesIO()
image.save(buffer, format='PNG')
image_data = base64.b64encode(buffer.getvalue()).decode()

# Make prediction
response = requests.post('http://localhost:5000/predict/classifier', 
                        json={'image': image_data})
result = response.json()
print(f"Predicted class: {result['predicted_class']}")
print(f"Confidence: {result['confidence']:.2%}")
```

## ğŸ¨ Dashboard Usage

### Streamlit Dashboard

```bash
cd app
streamlit run streamlit_app.py
```

The dashboard will be available at `http://localhost:8501`

### Features

- **Interactive Model Testing** - Upload images and get predictions
- **Real-time Visualizations** - Dynamic plots and charts
- **Model Comparison** - Side-by-side performance analysis
- **Data Exploration** - Interactive data visualization
- **Custom Data Upload** - Test with your own datasets

### Gradio Interface

```bash
cd app
python gradio_app.py
```

Alternative interface with different UI components.

## ğŸ”§ Advanced Usage

### Custom Model Training

```python
from scripts.train import train_model
from scripts.utils import load_mnist_data

# Load data
train_loader, val_loader, test_loader = load_mnist_data()

# Define custom model
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train model
model = MyModel()
trainer = train_model(model, train_loader, val_loader, 
                     task_type='classification', epochs=10)
```

### Custom Evaluation

```python
from scripts.evaluate import evaluate_model

# Evaluate model
results = evaluate_model(model, test_loader, 
                        task_type='classification',
                        save_dir='results/plots/my_model')

print(f"Accuracy: {results['accuracy']:.4f}")
print(f"F1-Score: {results['f1_score']:.4f}")
```

### Model Saving and Loading

```python
import torch

# Save model
torch.save(model.state_dict(), 'my_model.pth')

# Load model
model = MyModel()
model.load_state_dict(torch.load('my_model.pth'))
model.eval()
```

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### Adding New Experiments

1. Create a new notebook in `notebooks/`
2. Follow the naming convention: `XX_experiment_name.ipynb`
3. Include:
   - Clear problem statement
   - Dataset loading code
   - Model implementation
   - Training loop
   - Evaluation and visualization
4. Update documentation

### Reporting Issues

- Use GitHub Issues
- Include error messages and system information
- Provide minimal reproducible examples

### Code Style

- Follow PEP 8
- Use type hints where appropriate
- Add docstrings to functions
- Include comments for complex logic

## ğŸ› Troubleshooting

### Common Issues

**CUDA Out of Memory**
```python
# Use CPU instead
device = torch.device('cpu')
```

**Import Errors**
```bash
# Ensure all dependencies are installed
pip install -r requirements.txt
```

**Dataset Download Issues**
```python
# Set download=True and check internet connection
dataset = torchvision.datasets.MNIST('data', download=True)
```

**Jupyter Kernel Issues**
```bash
# Restart kernel and clear output
# Or restart Jupyter server
```

### Performance Tips

- Use GPU when available
- Adjust batch sizes based on memory
- Use mixed precision training for large models
- Enable cuDNN optimizations

### Getting Help

- Check the [experiments guide](experiments.md)
- Review error messages carefully
- Search existing issues
- Ask questions in discussions

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent framework
- The machine learning community for educational resources
- Contributors who help improve these experiments

---

**Happy Learning! ğŸ“**

Start with the first notebook and work your way through. Each experiment builds upon previous concepts while introducing new techniques and applications.
