{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05. Anomaly Detection\n",
        "\n",
        "This notebook implements anomaly detection using a Variational Autoencoder (VAE).\n",
        "\n",
        "## Experiment Overview\n",
        "- **Goal**: Detect anomalies in multivariate data using autoencoders\n",
        "- **Model**: Variational Autoencoder (VAE)\n",
        "- **Features**: Anomaly scoring, threshold optimization, ROC curves\n",
        "- **Learning**: Understanding unsupervised anomaly detection\n",
        "\n",
        "## What You'll Learn\n",
        "- Variational Autoencoders\n",
        "- Anomaly detection techniques\n",
        "- ROC curve analysis\n",
        "- Threshold optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Add scripts directory to path\n",
        "sys.path.append('../scripts')\n",
        "from utils import get_device, set_seed\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Get device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Generate synthetic data with anomalies\n",
        "print(\"Generating synthetic data with anomalies...\")\n",
        "# Normal data\n",
        "normal_data, _ = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=42, cluster_std=1.0)\n",
        "# Anomalous data\n",
        "anomaly_data, _ = make_blobs(n_samples=50, centers=1, n_features=2, random_state=42, cluster_std=0.3)\n",
        "anomaly_data += np.array([6, 6])  # Shift anomalies away from normal data\n",
        "\n",
        "# Combine data\n",
        "X = np.vstack([normal_data, anomaly_data])\n",
        "y = np.hstack([np.zeros(len(normal_data)), np.ones(len(anomaly_data))])  # 0=normal, 1=anomaly\n",
        "\n",
        "# Shuffle data\n",
        "indices = np.random.permutation(len(X))\n",
        "X, y = X[indices], y[indices]\n",
        "\n",
        "# Normalize data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"Normal samples: {np.sum(y == 0)}\")\n",
        "print(f\"Anomaly samples: {np.sum(y == 1)}\")\n",
        "\n",
        "# Visualize data\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', alpha=0.6, label='Anomaly')\n",
        "plt.title('Synthetic Data with Anomalies')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(y, bins=2, alpha=0.7, edgecolor='black')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class (0=Normal, 1=Anomaly)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1])\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Variational Autoencoder for anomaly detection\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=2, hidden_dim=32, latent_dim=2):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Latent space parameters\n",
        "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.logvar_layer = nn.Linear(hidden_dim, latent_dim)\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "        \n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.mu_layer(h)\n",
        "        logvar = self.logvar_layer(h)\n",
        "        return mu, logvar\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "# Create model instance\n",
        "model = VAE().to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(\"VAE Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for training (only use normal data for training)\n",
        "normal_indices = y == 0\n",
        "X_normal = X[normal_indices]\n",
        "y_normal = y[normal_indices]\n",
        "\n",
        "# Split normal data for training and validation\n",
        "train_size = int(0.8 * len(X_normal))\n",
        "X_train = X_normal[:train_size]\n",
        "X_val = X_normal[train_size:]\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train).to(device)\n",
        "X_val = torch.FloatTensor(X_val).to(device)\n",
        "X_all = torch.FloatTensor(X).to(device)\n",
        "\n",
        "print(f\"Training samples (normal only): {len(X_train)}\")\n",
        "print(f\"Validation samples (normal only): {len(X_val)}\")\n",
        "print(f\"Total samples for evaluation: {len(X)}\")\n",
        "\n",
        "# VAE Loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \"\"\"VAE loss = reconstruction loss + KL divergence.\"\"\"\n",
        "    # Reconstruction loss (MSE)\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    \n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return recon_loss + kl_loss\n",
        "\n",
        "# Training function\n",
        "def train_vae(model, X_train, X_val, epochs=100, lr=0.001):\n",
        "    \"\"\"Train VAE model.\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for i in range(0, len(X_train), 32):  # Batch size 32\n",
        "            batch = X_train[i:i+32]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            recon_batch, mu, logvar = model(batch)\n",
        "            loss = vae_loss(recon_batch, batch, mu, logvar)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(X_val), 32):\n",
        "                batch = X_val[i:i+32]\n",
        "                recon_batch, mu, logvar = model(batch)\n",
        "                loss = vae_loss(recon_batch, batch, mu, logvar)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        train_loss /= (len(X_train) // 32)\n",
        "        val_loss /= (len(X_val) // 32)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.2f}, Val Loss: {val_loss:.2f}')\n",
        "    \n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the VAE\n",
        "print(\"Starting VAE training...\")\n",
        "train_losses, val_losses = train_vae(model, X_train, X_val, epochs=100)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('VAE Training History')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training History (Log Scale)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log)')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/plots/vae_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Anomaly detection using reconstruction error\n",
        "def detect_anomalies(model, X, threshold_percentile=95):\n",
        "    \"\"\"Detect anomalies using reconstruction error.\"\"\"\n",
        "    model.eval()\n",
        "    reconstruction_errors = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X), 32):\n",
        "            batch = X[i:i+32]\n",
        "            recon_batch, _, _ = model(batch)\n",
        "            \n",
        "            # Calculate reconstruction error for each sample\n",
        "            mse = F.mse_loss(recon_batch, batch, reduction='none').sum(dim=1)\n",
        "            reconstruction_errors.extend(mse.cpu().numpy())\n",
        "    \n",
        "    reconstruction_errors = np.array(reconstruction_errors)\n",
        "    \n",
        "    # Set threshold based on percentile of reconstruction errors\n",
        "    threshold = np.percentile(reconstruction_errors, threshold_percentile)\n",
        "    \n",
        "    # Predict anomalies\n",
        "    predictions = (reconstruction_errors > threshold).astype(int)\n",
        "    \n",
        "    return reconstruction_errors, predictions, threshold\n",
        "\n",
        "# Detect anomalies\n",
        "print(\"Detecting anomalies...\")\n",
        "recon_errors, predictions, threshold = detect_anomalies(model, X_all, threshold_percentile=95)\n",
        "\n",
        "# Calculate metrics\n",
        "auc = roc_auc_score(y, recon_errors)\n",
        "print(f\"Anomaly Detection Results:\")\n",
        "print(f\"Threshold: {threshold:.4f}\")\n",
        "print(f\"AUC Score: {auc:.4f}\")\n",
        "print(f\"Detected anomalies: {np.sum(predictions)}\")\n",
        "print(f\"True anomalies: {np.sum(y)}\")\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Original data\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', alpha=0.6, label='True Anomaly')\n",
        "plt.title('Original Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Detected anomalies\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.scatter(X[predictions == 0, 0], X[predictions == 0, 1], c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(X[predictions == 1, 0], X[predictions == 1, 1], c='red', alpha=0.6, label='Detected Anomaly')\n",
        "plt.title('Detected Anomalies')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 3: Reconstruction errors\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c=recon_errors[y == 0], cmap='Blues', alpha=0.6, label='Normal')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c=recon_errors[y == 1], cmap='Reds', alpha=0.6, label='Anomaly')\n",
        "plt.colorbar(label='Reconstruction Error')\n",
        "plt.title('Reconstruction Errors')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "plt.subplot(2, 3, 4)\n",
        "fpr, tpr, _ = roc_curve(y, recon_errors)\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 5: Reconstruction error distribution\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.hist(recon_errors[y == 0], bins=30, alpha=0.7, label='Normal', color='blue')\n",
        "plt.hist(recon_errors[y == 1], bins=30, alpha=0.7, label='Anomaly', color='red')\n",
        "plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold: {threshold:.3f}')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Reconstruction Error Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 6: Latent space visualization\n",
        "plt.subplot(2, 3, 6)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    mu, _ = model.encode(X_all)\n",
        "    mu = mu.cpu().numpy()\n",
        "    \n",
        "plt.scatter(mu[y == 0, 0], mu[y == 0, 1], c='blue', alpha=0.6, label='Normal')\n",
        "plt.scatter(mu[y == 1, 0], mu[y == 1, 1], c='red', alpha=0.6, label='Anomaly')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('Latent Space')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/plots/anomaly_detection_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "cm = confusion_matrix(y, predictions)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y, predictions, target_names=['Normal', 'Anomaly']))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
