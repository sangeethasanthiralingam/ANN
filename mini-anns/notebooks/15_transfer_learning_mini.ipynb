{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 15. Transfer Learning Mini\n",
        "\n",
        "This notebook demonstrates transfer learning with tiny models.\n",
        "\n",
        "## Experiment Overview\n",
        "- **Goal**: Apply transfer learning to small datasets\n",
        "- **Model**: Pre-trained CNN with fine-tuning\n",
        "- **Features**: Feature extraction, fine-tuning, domain adaptation\n",
        "- **Learning**: Understanding transfer learning principles\n",
        "\n",
        "## What You'll Learn\n",
        "- Transfer learning concepts\n",
        "- Feature extraction vs fine-tuning\n",
        "- Domain adaptation\n",
        "- Pre-trained model utilization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Add scripts directory to path\n",
        "sys.path.append('../scripts')\n",
        "from utils import get_device, set_seed\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Get device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='../data/cifar10', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='../data/cifar10', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create small subset for transfer learning\n",
        "train_size = 1000\n",
        "test_size = 200\n",
        "train_indices = np.random.choice(len(train_dataset), train_size, replace=False)\n",
        "test_indices = np.random.choice(len(test_dataset), test_size, replace=False)\n",
        "\n",
        "train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
        "test_subset = torch.utils.data.Subset(test_dataset, test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_subset)}\")\n",
        "print(f\"Test samples: {len(test_subset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models for transfer learning comparison\n",
        "class TransferLearningCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, freeze_features=True):\n",
        "        super(TransferLearningCNN, self).__init__()\n",
        "        \n",
        "        # Feature extractor (pretrained-like)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        \n",
        "        # Freeze features if specified\n",
        "        if freeze_features:\n",
        "            for param in self.features.parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class FromScratchCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(FromScratchCNN, self).__init__()\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 4 * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, test_loader, epochs=20, lr=0.001, model_name=\"\"):\n",
        "    \"\"\"Train model and return training history.\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    train_losses = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # Test accuracy\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "        \n",
        "        accuracy = 100. * correct / total\n",
        "        test_accuracies.append(accuracy)\n",
        "        \n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'{model_name} - Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Acc: {accuracy:.2f}%')\n",
        "    \n",
        "    return train_losses, test_accuracies\n",
        "\n",
        "# Train models\n",
        "print(\"Training Transfer Learning Model (Frozen Features)...\")\n",
        "transfer_model = TransferLearningCNN(freeze_features=True).to(device)\n",
        "transfer_losses, transfer_accuracies = train_model(transfer_model, train_loader, test_loader, model_name=\"Transfer\")\n",
        "\n",
        "print(\"\\nTraining Transfer Learning Model (Fine-tuned)...\")\n",
        "finetune_model = TransferLearningCNN(freeze_features=False).to(device)\n",
        "finetune_losses, finetune_accuracies = train_model(finetune_model, train_loader, test_loader, model_name=\"Fine-tune\")\n",
        "\n",
        "print(\"\\nTraining From Scratch Model...\")\n",
        "scratch_model = FromScratchCNN().to(device)\n",
        "scratch_losses, scratch_accuracies = train_model(scratch_model, train_loader, test_loader, model_name=\"From Scratch\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(transfer_losses, label='Transfer Learning (Frozen)', linewidth=2)\n",
        "plt.plot(finetune_losses, label='Transfer Learning (Fine-tuned)', linewidth=2)\n",
        "plt.plot(scratch_losses, label='From Scratch', linewidth=2)\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(transfer_accuracies, label='Transfer Learning (Frozen)', linewidth=2)\n",
        "plt.plot(finetune_accuracies, label='Transfer Learning (Fine-tuned)', linewidth=2)\n",
        "plt.plot(scratch_accuracies, label='From Scratch', linewidth=2)\n",
        "plt.title('Test Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "# Final accuracies\n",
        "models = ['Transfer (Frozen)', 'Transfer (Fine-tuned)', 'From Scratch']\n",
        "final_accuracies = [transfer_accuracies[-1], finetune_accuracies[-1], scratch_accuracies[-1]]\n",
        "plt.bar(models, final_accuracies)\n",
        "plt.title('Final Test Accuracy')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Training efficiency\n",
        "efficiency = [acc / loss for acc, loss in zip(final_accuracies, [transfer_losses[-1], finetune_losses[-1], scratch_losses[-1]])]\n",
        "plt.bar(models, efficiency)\n",
        "plt.title('Training Efficiency (Accuracy/Loss)')\n",
        "plt.ylabel('Efficiency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/plots/transfer_learning_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nTransfer Learning Summary:\")\n",
        "print(f\"Transfer Learning (Frozen): {transfer_accuracies[-1]:.2f}%\")\n",
        "print(f\"Transfer Learning (Fine-tuned): {finetune_accuracies[-1]:.2f}%\")\n",
        "print(f\"From Scratch: {scratch_accuracies[-1]:.2f}%\")\n",
        "\n",
        "# Save models\n",
        "torch.save(transfer_model.state_dict(), '../results/logs/transfer_model.pth')\n",
        "torch.save(finetune_model.state_dict(), '../results/logs/finetune_model.pth')\n",
        "torch.save(scratch_model.state_dict(), '../results/logs/scratch_model.pth')\n",
        "\n",
        "print(\"\\nModels saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
