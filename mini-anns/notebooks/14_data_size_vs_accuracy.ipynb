{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14. Data Size vs Accuracy\n",
        "\n",
        "This notebook explores the relationship between dataset size and model accuracy.\n",
        "\n",
        "## Experiment Overview\n",
        "- **Goal**: Analyze how dataset size affects model performance\n",
        "- **Model**: MLP trained on different data subsets\n",
        "- **Features**: Learning curves, data efficiency analysis, scaling laws\n",
        "- **Learning**: Understanding data requirements for neural networks\n",
        "\n",
        "## What You'll Learn\n",
        "- Data efficiency in neural networks\n",
        "- Learning curves and scaling laws\n",
        "- Sample complexity analysis\n",
        "- Data augmentation effects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "\n",
        "# Add scripts directory to path\n",
        "sys.path.append('../scripts')\n",
        "from utils import load_mnist_data, get_device, set_seed\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Get device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "print(\"Loading MNIST dataset...\")\n",
        "train_loader, val_loader, test_loader = load_mnist_data(batch_size=64, test_split=0.2)\n",
        "\n",
        "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define MLP model\n",
        "class DataSizeMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=128, num_classes=10):\n",
        "        super(DataSizeMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
        "    \"\"\"Train model and return final accuracy.\"\"\"\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            accuracy = 100. * correct / total\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Val Accuracy: {accuracy:.2f}%')\n",
        "    \n",
        "    # Final accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "    \n",
        "    return 100. * correct / total\n",
        "\n",
        "# Test different data sizes\n",
        "data_sizes = [100, 500, 1000, 2000, 5000, 10000, 20000, 40000]\n",
        "accuracies = []\n",
        "\n",
        "print(\"Testing different data sizes...\")\n",
        "for size in data_sizes:\n",
        "    print(f\"\\nTraining with {size} samples...\")\n",
        "    \n",
        "    # Create subset\n",
        "    indices = np.random.choice(len(train_loader.dataset), size, replace=False)\n",
        "    subset = Subset(train_loader.dataset, indices)\n",
        "    subset_loader = DataLoader(subset, batch_size=64, shuffle=True)\n",
        "    \n",
        "    # Train model\n",
        "    model = DataSizeMLP().to(device)\n",
        "    accuracy = train_model(model, subset_loader, val_loader, epochs=20)\n",
        "    accuracies.append(accuracy)\n",
        "    \n",
        "    print(f\"Final accuracy with {size} samples: {accuracy:.2f}%\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(data_sizes, accuracies, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Training Samples')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.title('Accuracy vs Data Size')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(np.log10(data_sizes), accuracies, 'ro-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Log10(Training Samples)')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.title('Accuracy vs Log Data Size')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "# Power law fit\n",
        "log_sizes = np.log10(data_sizes)\n",
        "log_accuracies = np.log10(accuracies)\n",
        "z = np.polyfit(log_sizes, log_accuracies, 1)\n",
        "p = np.poly1d(z)\n",
        "plt.plot(log_sizes, log_accuracies, 'go-', linewidth=2, markersize=8, label='Data')\n",
        "plt.plot(log_sizes, p(log_sizes), 'r--', linewidth=2, label=f'Fit: y={z[0]:.2f}x+{z[1]:.2f}')\n",
        "plt.xlabel('Log10(Training Samples)')\n",
        "plt.ylabel('Log10(Accuracy)')\n",
        "plt.title('Power Law Fit')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "# Efficiency analysis\n",
        "efficiency = [acc / size for acc, size in zip(accuracies, data_sizes)]\n",
        "plt.plot(data_sizes, efficiency, 'mo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Training Samples')\n",
        "plt.ylabel('Accuracy per Sample')\n",
        "plt.title('Data Efficiency')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../results/plots/data_size_vs_accuracy.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\nData Size vs Accuracy Summary:\")\n",
        "for size, acc in zip(data_sizes, accuracies):\n",
        "    print(f\"{size:5d} samples: {acc:6.2f}% accuracy\")\n",
        "\n",
        "print(f\"\\nPower law coefficient: {z[0]:.3f}\")\n",
        "print(f\"R-squared: {np.corrcoef(log_sizes, log_accuracies)[0,1]**2:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
