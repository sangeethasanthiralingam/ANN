{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Micro LSTM\n",
        "\n",
        "This notebook implements a minimal LSTM for character-level text generation.\n",
        "\n",
        "## Experiment Overview\n",
        "- **Goal**: Generate text using a minimal LSTM\n",
        "- **Model**: Single-layer LSTM with embedding layer\n",
        "- **Features**: Character-level text generation, training on simple sequences\n",
        "- **Learning**: Understanding recurrent neural networks and sequence modeling\n",
        "\n",
        "## What You'll Learn\n",
        "- Building LSTM architectures\n",
        "- Character-level text processing\n",
        "- Sequence generation and sampling\n",
        "- Training recurrent networks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add scripts directory to path\n",
        "sys.path.append('../scripts')\n",
        "from utils import get_device, set_seed\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Get device\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Sample text data for training\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "Machine learning is fascinating and powerful.\n",
        "Neural networks can learn complex patterns.\n",
        "Deep learning has revolutionized AI.\n",
        "Artificial intelligence is the future.\n",
        "\"\"\"\n",
        "\n",
        "# Create character mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Characters: {chars}\")\n",
        "print(f\"Text length: {len(text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Micro LSTM model\n",
        "class MicroLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=64, embedding_size=32):\n",
        "        super(MicroLSTM, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
        "        \n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, hidden=None):\n",
        "        # Embed input\n",
        "        embedded = self.embedding(x)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "        \n",
        "        # Output layer\n",
        "        output = self.fc(lstm_out)\n",
        "        \n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "# Create model instance\n",
        "model = MicroLSTM(vocab_size).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare training data\n",
        "def create_sequences(text, seq_length=20):\n",
        "    \"\"\"Create input-output sequences for training.\"\"\"\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    \n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = text[i:i + seq_length]\n",
        "        target = text[i + seq_length]\n",
        "        \n",
        "        # Convert to indices\n",
        "        seq_idx = [char_to_idx[ch] for ch in seq]\n",
        "        target_idx = char_to_idx[target]\n",
        "        \n",
        "        sequences.append(seq_idx)\n",
        "        targets.append(target_idx)\n",
        "    \n",
        "    return torch.tensor(sequences), torch.tensor(targets)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 20\n",
        "X, y = create_sequences(text, seq_length)\n",
        "\n",
        "print(f\"Number of sequences: {len(X)}\")\n",
        "print(f\"Sequence length: {seq_length}\")\n",
        "print(f\"Input shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Show example sequence\n",
        "print(f\"\\nExample sequence:\")\n",
        "print(f\"Input:  {''.join([idx_to_char[idx.item()] for idx in X[0]])}\")\n",
        "print(f\"Target: {idx_to_char[y[0].item()]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the LSTM\n",
        "def train_lstm(model, X, y, epochs=100, lr=0.01, batch_size=32):\n",
        "    \"\"\"Train the LSTM model.\"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        # Mini-batch training\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            batch_X = X[i:i+batch_size].to(device)\n",
        "            batch_y = y[i:i+batch_size].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            output, _ = model(batch_X)\n",
        "            loss = criterion(output.view(-1, model.vocab_size), batch_y)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        avg_loss = total_loss / (len(X) // batch_size)\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    return losses\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting LSTM training...\")\n",
        "losses = train_lstm(model, X, y, epochs=100, lr=0.01)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('LSTM Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.savefig('../results/plots/lstm_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text generation function\n",
        "def generate_text(model, start_text, length=100, temperature=1.0):\n",
        "    \"\"\"Generate text using the trained model.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Convert start text to indices\n",
        "    start_indices = [char_to_idx[ch] for ch in start_text]\n",
        "    generated = start_indices.copy()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Initialize hidden state\n",
        "        hidden = model.init_hidden(1)\n",
        "        \n",
        "        # Feed the start sequence\n",
        "        for idx in start_indices:\n",
        "            input_tensor = torch.tensor([[idx]]).to(device)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "        \n",
        "        # Generate new characters\n",
        "        for _ in range(length):\n",
        "            # Get probabilities\n",
        "            probs = F.softmax(output[0, -1, :] / temperature, dim=0)\n",
        "            \n",
        "            # Sample next character\n",
        "            next_char_idx = torch.multinomial(probs, 1).item()\n",
        "            generated.append(next_char_idx)\n",
        "            \n",
        "            # Use the generated character as next input\n",
        "            input_tensor = torch.tensor([[next_char_idx]]).to(device)\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "    \n",
        "    # Convert back to text\n",
        "    generated_text = ''.join([idx_to_char[idx] for idx in generated])\n",
        "    return generated_text\n",
        "\n",
        "# Generate text with different temperatures\n",
        "print(\"Generating text with different temperatures:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_text = \"The quick brown\"\n",
        "for temp in [0.5, 1.0, 1.5]:\n",
        "    generated = generate_text(model, start_text, length=50, temperature=temp)\n",
        "    print(f\"Temperature {temp}: {generated}\")\n",
        "    print()\n",
        "\n",
        "# Generate multiple samples\n",
        "print(\"Multiple generation samples:\")\n",
        "print(\"=\" * 50)\n",
        "for i in range(3):\n",
        "    generated = generate_text(model, \"Machine learning\", length=30, temperature=1.0)\n",
        "    print(f\"Sample {i+1}: {generated}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
